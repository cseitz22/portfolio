---
title: "Portfolio 6 - March Madness Regression"
name: "Cat Seitz"
date: "1.21.2023"
data: "Distractor Suppression Pilot Study"
output: 
  html_document: 
    theme: cerulean
---

Goal: Create a model for predicting how far in March Madness a team will go. 
Product: The perfect March Madness bracket.  
Data: 2022 team stats on season win percentage, conference difficulty, times the team has made it to each round of the post-season, bracket rank, and power rankings from Bleacher Report. 
Interpretation: I predict Kansas and Duke will play in the 2023 championship game and Kansas will win based on my model. 


I would like to recognize that 1 year of post-season data is not great for predicting future brackets, but it took awhile to get all this data into a usable form, so it's what we'll work with for this portfolio and my bracket this year. 

```{r load-packages, message=FALSE}
library(tidyverse) 
library(tidyr)
library(tidytuesdayR)
library(tidymodels)
library(openintro)
library(broom)
```


```{r import-2022-data}

mm22 <- read_csv("docs/2022marchmadness.csv", show_col_types = FALSE)
  
```

First, we'll check out how well the season percent wins predicts the round the team lost in the post-season. This is unlikely to be very predictive because the teams have a large range of schedule difficulties, meaning that some teams play the best teams all season and others play average teams all season. It's likely that teams that play better teams in their regular season will go further in the post season. 

```{r plot-pct-wins}


ggplot(mm22, aes(y = round_lost, x=pct_wins))+
  geom_point()+
  stat_smooth(method="lm")


```

```{r fit-linear-model-pct-wins}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ pct_wins, data = mm22) %>%
  tidy()

summary(lm(round_lost ~ pct_wins, data = mm22))
```

As predicted, percent wins is not a great predictor of how far a team will go in the post-season. Next, we'll take a look at the difficulty of their conference. In hindsight, I should have scored difficulty in the opposite direction because lower scores indicate more difficult. This is because I scored as rankings of most difficult, but this shouldn't affect how the model works. Conference difficulty isn't too bad at predicting the round a team lost. The more difficulty a team's conference is, the further they go in the post-season tournament. 

```{r plot-round-lost}


ggplot(mm22, aes(y = round_lost, x=strength_schedule))+
  geom_point()+
  stat_smooth(method="lm")


```



```{r fit-linear-model-conference-difficulty}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ strength_schedule, data = mm22) %>%
  tidy()

summary(lm(round_lost ~ strength_schedule, data=mm22))

```

Next, we'll explore how a team's bracket rank and their power ranking predict how far they go in the tournament. These two rankings account for 23% of the variance in the round a team lost. 

```{r fit-linear-model-ranks}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ overall_rank + bracket_rank, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ overall_rank + bracket_rank, data = mm22))

```

Now, we'll look at how a team's history of making it to further round of the tournament (starting in the year 2000) will predict how far they will go in the tournament in the current year. This accounts for 41% of the variance in the round a team lost. 

```{r fit-linear-model-post-season-times}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ x_sweet16 + x_elite8 + x_final4 + x_runner + x_won, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ x_sweet16 + x_elite8 + x_final4 + x_runner + x_won, data = mm22))

```

Here, we'll take a look at freethrow percentage, points earned and points allowed. Freethrow percentage isn't related to round lost. The difference in points earned and points allowed seems to somewhat help predict what round a team lost. 


```{r plot-round-diff-pts}


ggplot(mm22, aes(y = round_lost, x=diff_pts))+
  geom_point()+
  stat_smooth(method="lm")


```


```{r fit-linear-modelfreethrow}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ ft_pct, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ ft_pct, data = mm22))

```
```{r fit-linear-model-diff-pts}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ diff_pts, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ diff_pts, data = mm22))

```


Now, I'm going to add some of these variables together to make the best model. Using backward-selection, I came to the following model for predicting the round a team will lose. 

Round Lost = 2.80 +  .62(Runner up times) + .65(Champ times) -.025(power rank) - .045(mean difference in points)

The intercept is not super helpful because it predicts the round a team will lose if they have never made it to championship game but are ranked basically in 1st, which is pretty much not ever going to happen. This model accounts for 42.8% of variance in the the round a team loses. 


```{r fit-linear-model-final}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ x_runner + x_won + overall_rank+ diff_pts, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ x_runner + x_won + overall_rank+ diff_pts, data = mm22))

```

Here, I loaded in the 2023 team data and predicted the round each team will lose and I can sort by highest to lowest and build my bracket. Overall, this isn't a super insightful model because it is largely using the power rankings from Bleacher Report which must take in a bunch of other data to create their ranking, and past championship appearances from each team, which doesn't totally mean much to how well a team will do in the current season. Nevertheless, we will see how far this model will get me in my March Madness pools. Based on this model, Kansas and Duke will play in the final and Kansas will win. This model still doesn't account for teams being in the same side of the bracket and not necessarily going as far as they should, but I will not be able to deal with that here. 

```{r import-2023-data}

mm23 <- read_csv("docs/2023marchmadness.csv", show_col_types = FALSE)
  
```


```{r add-predicted-round-lost}

mm23<-mm23 %>%
  mutate(round_lost = 2.8 + (.62*x_runner) + (.65*x_won) - (.025*overall_rank) - (.045*diff_pts))

```

```{r show-best-teams}

mm23 %>%                                      # Top N highest values by group
  arrange(desc(round_lost)) %>%
  slice(1:64)
```

Lastly, I wanted to test out a model that doesn't necessarily take into account previous win history. This model only accounts for 22.5% of the variance in the round a team lost, so doesn't seem to be as good as the previous model but I'll use it for a different bracket and see which one wins. 


```{r fit-linear-model-final2}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ strength_schedule+ overall_rank+ ft_pct+ pts_earned+pts_allowed+bracket_rank, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ strength_schedule+ overall_rank+ ft_pct+ pts_earned+pts_allowed+bracket_rank, data = mm22))

```



```{r add-predicted-round-lost2}

mm23<-mm23 %>%
  mutate(round_lost2 = .269 + (.06*ft_pct) - (.06*pts_earned) + (.05*pts_allowed) - (.09*overall_rank)+ (.16*bracket_rank))


mm23 %>%                                      # Top N highest values by group
  arrange(desc(round_lost2)) %>%
  slice(1:64)

```

Second lastly, instead of the round the team lost as the predictor, I ranked the teams by the outcome of March Madness from 2022 by each round and how badly they lost. We'll see how this ranking is predicted based on the same variables. The first model takes into account the strength of the team's schedule, their power rankings, free throw percentage, points earned, points allowed, and their bracket ranking. 

Model: Final ranking = -2.13 + .022*strength_schedule + .81*overall_rank - .43*ft_pct+ .5pts_earned+.13pts_allowed - 1.05*bracket_rank



```{r fit-linear-model-final3}

linear_reg() %>%
  set_engine("lm") %>%
  fit(final_ranking ~ strength_schedule + overall_rank + 
    ft_pct + pts_earned + pts_allowed + bracket_rank, data = mm22) %>%
  tidy()
summary(lm(final_ranking ~ strength_schedule + overall_rank + 
    ft_pct + pts_earned + pts_allowed + bracket_rank, data = mm22))

```

```{r add-predicted-round-lost3}

mm23<-mm23 %>%
  mutate(round_lost3 = -2.145+ (.022*strength_schedule) +(.81*overall_rank)-(.43*ft_pct)+(.5*pts_earned)+(.13*pts_allowed)- (1.05*bracket_rank))


mm23 %>%                                      # Top N highest values by group
  arrange((round_lost3)) %>%
  slice(1:64)

```



The second model takes into account the strength of schedule, times as runner up, times winning, and their power ranking. 

Model: final ranking = 18.49 + .033*strength_schedule - 4.96*x_runner - 2.17*x_won + .39*overall_rank


```{r fit-linear-model-final4}

linear_reg() %>%
  set_engine("lm") %>%
  fit(final_ranking ~ strength_schedule+x_runner + x_won + overall_rank, data = mm22) %>%
  tidy()
summary(lm(final_ranking ~strength_schedule+ x_runner + x_won + overall_rank, data = mm22))

```

```{r add-predicted-round-lost4}

mm23<-mm23 %>%
  mutate(round_lost4 = 18.49 + (.033*strength_schedule)- (4.96*x_runner) - (2.17*x_won) + (.39*overall_rank))

mm23 %>%                                      # Top N highest values by group
  arrange((round_lost4)) %>%
  slice(1:64)
```







