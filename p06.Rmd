---
title: "Portfolio 6 - March Madness Regression"
name: "Cat Seitz"
date: "1.21.2023"
data: "Distractor Suppression Pilot Study"
output: 
  html_document: 
    theme: cerulean
---

Goal: Create a model for predicting how far in March Madness a team will go. 
Product: The perfect March Madness bracket.  
Data: 2022 team stats on season win percentage, conference difficulty, times the team has made it to each round of the post-season, bracket rank, and power rankings from Bleacher Report. 
Interpretation: 


I would like to recognize that 1 year of post-season data is not great for predicting future brackets, but it took awhile to get all this data into a usable form, so it's what we'll work with for this portfolio and my bracket this year. 

```{r load-packages, message=FALSE}
library(tidyverse) 
library(tidyr)
library(tidytuesdayR)
library(tidymodels)
library(openintro)
library(broom)
```


```{r import-2022-data}

mm22 <- read_csv("docs/2022marchmadness.csv", show_col_types = FALSE)
  
```

First, we'll check out how well the season percent wins predicts the round the team lost in the post-season. This is unlikely to be very predictive because the teams have a large range of schedule difficulties, meaning that some teams play the best teams all season and others play average teams all season. It's likely that teams that play better teams in their regular season will go further in the post season. 

```{r plot-round-lost}


ggplot(mm22, aes(y = round_lost, x=pct_wins))+
  geom_point()+
  stat_smooth(method="lm")
#  labs(title="Relationship between Cocoa Percentage and Rating", y="Rating", x="Cocoa Percentage")


```

```{r fit-linear-model-pct-wins}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ pct_wins, data = mm22) %>%
  tidy()

summary(lm(round_lost ~ pct_wins, data = mm22))
```

As predicted, percent wins is not a great predictor of how far a team will go in the post-season. Next, we'll take a look at the difficulty of their conference. In hindsight, I should have scored difficulty in the opposite direction because lower scores indicate more difficult. This is because I scored as rankings of most difficult, but this shouldn't affect how the model works. Conference difficulty isn't too bad at predicting the round a team lost. The more difficulty a team's conference is, the further they go in the post-season tournament. 

```{r plot-round-lost}


ggplot(mm22, aes(y = round_lost, x=conference_difficulty))+
  geom_point()+
  stat_smooth(method="lm")
#  labs(title="Relationship between Cocoa Percentage and Rating", y="Rating", x="Cocoa Percentage")


```

```{r fit-linear-model-conference-difficulty}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ conference_difficulty, data = mm22) %>%
  tidy()

summary(lm(round_lost ~ conference_difficulty, data=mm22))

```

Next, we'll explore how a team's bracket rank and their power ranking predict how far they go in the tournament. These two rankings account for 23% of the variance in the round a team lost. 

```{r fit-linear-model-ranks}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ overall_rank + bracket_rank, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ overall_rank + bracket_rank, data = mm22))

```

Now, we'll look at how a team's history of making it to further round of the tournament (starting in the year 2000) will predict how far they will go in the tournament in the current year. This accounts for 41% of the variance in the round a team lost. 

```{r fit-linear-model-post-season-times}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ x_sweet16 + x_elite8 + x_final4 + x_runner + x_won, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ x_sweet16 + x_elite8 + x_final4 + x_runner + x_won, data = mm22))

```

Now, I'm going to add some of these variables together to make the best model. Using backward-selection, I came to the following model for predicting the round a team will lose. 

Round Lost = 1.96 +  .42(Runner up times) + .52(Champ times) -.014(power rank)

The intercept is not super helpful because it predicts the round a team will lose if they have never made it to championship game but are ranked basically in 1st, which is pretty much not ever going to happen. This model accounts for 42.4% of variance in the 


```{r fit-linear-model-final}

linear_reg() %>%
  set_engine("lm") %>%
  fit(round_lost ~ x_runner + x_won + overall_rank, data = mm22) %>%
  tidy()
summary(lm(round_lost ~ x_runner + x_won + overall_rank, data = mm22))

```

Here, I loaded in the 2023 team data and predicted the round each team will lose and I can sort by highest to lowest and build my bracket. Overall, this isn't a super insightful model because it is largely using the power rankings from Bleacher Report which must take in a bunch of other data to create their ranking, and past championship appearances from each team, which doesn't totally mean much to how well a team will do in the current season. Nevertheless, we will see how far this model will get me in my March Madness pools. Based on this model, Kansas and UConn will play in the final and Kansas will win. Unfortunately, Kansas and UConn are in the same region so this is impossible -- my model could not take this into account, so I'll have to deal with this a different way. I could try to 

```{r import-2023-data}

mm23 <- read_csv("docs/2023marchmadness.csv", show_col_types = FALSE)
  
```

```{r add-predicted-round-lost}

mm23<-mm23 %>%
  mutate(round_lost<- 1.96 + (.42*x_runner) + (.52*x_won) - (.014*overall_rank))

```


